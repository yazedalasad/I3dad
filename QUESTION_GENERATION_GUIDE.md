# ðŸ“š Question Generation Guide - DeepSeek API Integration

## Overview

This guide explains how to use the automated question generation system to parse Israeli psychometric exams (2015-2025) and populate your question bank with IRT-calibrated questions.

---

## ðŸš€ Quick Start

### 1. Setup

```bash
# Install required dependencies
npm install axios

# Create exams folder
mkdir psychometric_exams

# Set DeepSeek API key
export DEEPSEEK_API_KEY="your-api-key-here"
```

### 2. Add Exam PDFs

Place your psychometric exam PDFs in the `psychometric_exams` folder:

```
psychometric_exams/
â”œâ”€â”€ math_2015.pdf
â”œâ”€â”€ math_2016.pdf
â”œâ”€â”€ english_2017.pdf
â”œâ”€â”€ physics_2018.pdf
â”œâ”€â”€ chemistry_2019.pdf
â”œâ”€â”€ biology_2020.pdf
â”œâ”€â”€ history_2021.pdf
â”œâ”€â”€ literature_2022.pdf
â”œâ”€â”€ hebrew_2023.pdf
â”œâ”€â”€ citizenship_2024.pdf
â””â”€â”€ computer_science_2025.pdf
```

### 3. Run Question Generator

```bash
# Run the generator
node scripts/questionGenerator.js
```

---

## ðŸ“‹ What the System Does

### Step 1: PDF Text Extraction
- Reads each PDF file
- Uses DeepSeek API to extract all text
- Preserves structure and formatting
- Extracts questions, options, and answers

### Step 2: Subject Identification
- Analyzes content and keywords
- Matches to one of 10 subjects:
  - Mathematics (MATH)
  - English (ENG)
  - Hebrew (HEB)
  - History (HIST)
  - Literature (LIT)
  - Citizenship (CIV)
  - Physics (PHYS)
  - Chemistry (CHEM)
  - Biology (BIO)
  - Computer Science (CS)

### Step 3: Question Parsing
- Extracts each question with 4 options
- Identifies correct answers
- Translates to Arabic and Hebrew (if needed)
- Determines cognitive level
- Estimates initial difficulty

### Step 4: IRT Parameter Estimation
Uses DeepSeek to estimate:
- **Difficulty (b)**: -3.0 to +3.0
  - -3.0 to -1.5: Very easy
  - -1.5 to -0.5: Easy
  - -0.5 to 0.5: Medium
  - 0.5 to 1.5: Hard
  - 1.5 to 3.0: Very hard

- **Discrimination (a)**: 0.5 to 2.5
  - How well the question differentiates ability levels

- **Guessing (c)**: 0.20 to 0.30
  - Probability of correct answer by random guessing

### Step 5: Database Storage
- Saves all questions to Supabase
- Links to appropriate subject
- Includes bilingual text
- Stores IRT parameters

---

## ðŸ”§ Configuration

### Environment Variables

Create a `.env` file:

```env
DEEPSEEK_API_KEY=your-deepseek-api-key
SUPABASE_URL=your-supabase-url
SUPABASE_KEY=your-supabase-key
```

### Folder Structure

```
project/
â”œâ”€â”€ psychometric_exams/          # Place PDFs here
â”‚   â”œâ”€â”€ math_2015.pdf
â”‚   â”œâ”€â”€ english_2016.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ questionGenerator.js     # Main generator script
â”œâ”€â”€ config/
â”‚   â””â”€â”€ supabase.js             # Supabase config
â””â”€â”€ .env                         # Environment variables
```

---

## ðŸ“Š Expected Output

### Console Output

```
ðŸš€ Starting Question Generation Process...

ðŸ“š Found 11 exam files:

   1. math_2015.pdf
   2. math_2016.pdf
   3. english_2017.pdf
   ...

âœ… Loaded 10 subjects from database

============================================================
ðŸ“„ Processing: math_2015.pdf (1/11)
============================================================
ðŸ“– Extracting text from PDF...
âœ… Extracted 15234 characters
ðŸ” Identifying subject...
âœ… Identified subject: Mathematics (MATH)
ðŸ”¨ Parsing questions...
âœ… Parsed 45 questions
ðŸ“Š Estimating IRT parameters...
âœ… Added IRT parameters to 45 questions
ðŸ’¾ Saving to database...
âœ… Saved 45 questions to database

============================================================
ðŸŽ‰ COMPLETE! Generated 450 total questions
============================================================
```

### Database Result

Questions will be saved with:
- âœ… Bilingual text (Arabic/Hebrew)
- âœ… 4 multiple choice options
- âœ… Correct answer
- âœ… IRT parameters (difficulty, discrimination, guessing)
- âœ… Cognitive level
- âœ… Subject linkage

---

## ðŸŽ¯ Question Format

### Input (PDF)
```
Question 15:
What is the derivative of f(x) = 3xÂ² + 2x - 1?

A) 6x + 2
B) 3x + 2
C) 6xÂ² + 2x
D) 3xÂ² + 2

Answer: A
```

### Output (Database)
```json
{
  "subject_id": "uuid-for-math",
  "question_text_ar": "Ù…Ø§ Ù‡Ùˆ Ù…Ø´ØªÙ‚ Ø§Ù„Ø¯Ø§Ù„Ø© f(x) = 3xÂ² + 2x - 1ØŸ",
  "question_text_he": "×ž×” ×”× ×’×–×¨×ª ×©×œ ×”×¤×•× ×§×¦×™×” f(x) = 3xÂ² + 2x - 1?",
  "option_a_ar": "6x + 2",
  "option_a_he": "6x + 2",
  "option_b_ar": "3x + 2",
  "option_b_he": "3x + 2",
  "option_c_ar": "6xÂ² + 2x",
  "option_c_he": "6xÂ² + 2x",
  "option_d_ar": "3xÂ² + 2",
  "option_d_he": "3xÂ² + 2",
  "correct_answer": "A",
  "difficulty": 1.2,
  "discrimination": 1.8,
  "guessing": 0.25,
  "cognitive_level": "application",
  "target_language": "both",
  "is_active": true
}
```

---

## ðŸ” Quality Control

### Automatic Validation

The system validates:
- âœ… All 4 options are present
- âœ… Correct answer is A, B, C, or D
- âœ… IRT parameters are in valid ranges
- âœ… Both language versions exist
- âœ… Subject is correctly identified

### Manual Review Recommended

After generation, review:
1. **Translation Quality**: Check Arabic/Hebrew translations
2. **IRT Parameters**: Verify difficulty estimates
3. **Correct Answers**: Confirm answer keys
4. **Subject Classification**: Ensure proper categorization

---

## ðŸ“ˆ Performance Expectations

### Processing Time
- **Per PDF**: 2-5 minutes
- **Per Question**: 5-10 seconds
- **Total (11 PDFs)**: 30-60 minutes

### Question Yield
- **Expected**: 30-50 questions per exam
- **Total**: 300-500 questions from 11 exams
- **Success Rate**: 85-95%

### API Usage
- **DeepSeek API Calls**: ~4 per PDF
- **Tokens Used**: ~20,000 per PDF
- **Cost Estimate**: $0.50-$1.00 per PDF

---

## ðŸ› ï¸ Troubleshooting

### Issue: "No PDF files found"
**Solution**: Add PDF files to `psychometric_exams` folder

### Issue: "DeepSeek API Error"
**Solution**: Check API key in `.env` file

### Issue: "Could not identify subject"
**Solution**: 
- Check PDF content quality
- Ensure text is extractable
- Manually specify subject if needed

### Issue: "No questions parsed"
**Solution**:
- Verify PDF format
- Check if questions are in standard format
- Try with a different PDF

### Issue: "Database save failed"
**Solution**:
- Verify Supabase connection
- Check if subjects table is populated
- Ensure schema is deployed

---

## ðŸ”„ Batch Processing

### Process Multiple Years

```bash
# Process all exams from 2015-2025
node scripts/questionGenerator.js

# Process specific subject
# (modify script to filter by filename pattern)
```

### Resume After Interruption

The script processes files sequentially. If interrupted:
1. Check database for already-processed questions
2. Remove processed PDFs from folder
3. Re-run script for remaining files

---

## ðŸ“ Best Practices

### 1. Organize PDFs by Subject
```
psychometric_exams/
â”œâ”€â”€ math/
â”‚   â”œâ”€â”€ 2015.pdf
â”‚   â”œâ”€â”€ 2016.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ english/
â”‚   â”œâ”€â”€ 2015.pdf
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

### 2. Name Files Descriptively
```
subject_year_version.pdf
Examples:
- math_2015_winter.pdf
- english_2016_summer.pdf
- physics_2017_makeup.pdf
```

### 3. Verify Before Mass Import
- Test with 1-2 PDFs first
- Review generated questions
- Adjust prompts if needed
- Then process all files

### 4. Backup Database
```bash
# Before running generator
pg_dump your_database > backup.sql
```

---

## ðŸŽ“ IRT Parameter Calibration

### Initial Estimates
DeepSeek provides initial estimates based on:
- Content complexity
- Cognitive level
- Subject matter
- Question structure

### Refinement Over Time
As students answer questions:
1. Collect response data
2. Run IRT calibration algorithms
3. Update parameters in database
4. Improve accuracy

### Calibration Script (Future)
```bash
# After collecting 100+ responses per question
node scripts/calibrateIRT.js
```

---

## ðŸ“Š Monitoring Progress

### Check Database
```sql
-- Count questions by subject
SELECT 
  s.name_en,
  COUNT(q.id) as question_count
FROM subjects s
LEFT JOIN questions q ON s.id = q.subject_id
GROUP BY s.name_en
ORDER BY question_count DESC;

-- Check IRT parameter distribution
SELECT 
  CASE 
    WHEN difficulty < -1.5 THEN 'Very Easy'
    WHEN difficulty < -0.5 THEN 'Easy'
    WHEN difficulty < 0.5 THEN 'Medium'
    WHEN difficulty < 1.5 THEN 'Hard'
    ELSE 'Very Hard'
  END as difficulty_level,
  COUNT(*) as count
FROM questions
GROUP BY difficulty_level;
```

---

## ðŸš€ Next Steps

After question generation:

1. âœ… **Review Questions**: Manual quality check
2. âœ… **Test System**: Run sample adaptive tests
3. âœ… **Collect Data**: Gather student responses
4. âœ… **Calibrate IRT**: Refine parameters
5. âœ… **Deploy**: Launch to production

---

## ðŸ“ž Support

### Common Questions

**Q: How accurate are the IRT parameters?**
A: Initial estimates are 70-80% accurate. They improve with real student data.

**Q: Can I edit questions after generation?**
A: Yes, use Supabase dashboard to edit any question.

**Q: What if translation is incorrect?**
A: Edit directly in database or re-run with improved prompts.

**Q: How to add more subjects?**
A: Add to subjects table, update SUBJECT_MAPPING in script.

---

## ðŸŽ‰ Success Metrics

After running the generator, you should have:
- âœ… 300-500 questions across 10 subjects
- âœ… All questions with IRT parameters
- âœ… Bilingual support (Arabic/Hebrew)
- âœ… Ready for adaptive testing
- âœ… Calibrated difficulty levels

**Your question bank is now ready for intelligent adaptive testing!** ðŸš€
